{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp -r logs logs_151121_fix_pos_emb_search_param\n",
    "# !cp -r checkpoints checkpoints_151121_fix_pos_emb_search_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882446a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -zcvf checkpoints.tar.gz checkpoints\n",
    "# !mv checkpoints.tar.gz checkpoints_101121_no_pos_emb.tar.gz\n",
    "\n",
    "# ! tar -zcvf logs_101121_no_pos_emb.tar.gz logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cbfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r logs\n",
    "# !rm -r checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46293b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir logs\n",
    "# !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9dfb631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from x_transformers.x_transformers import XTransformer\n",
    "import torch\n",
    "\n",
    "from run_experiment import *\n",
    "from generate_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e6986",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de49706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "TAG = '10tkn_len24_ext'\n",
    "\n",
    "TASK_NAME = 'copy'\n",
    "TRAIN_SIZE = 100_000\n",
    "VAL_SIZE = 2_000\n",
    "TEST_SIZE = 10_000\n",
    "NUM_INITS = 4\n",
    "\n",
    "\n",
    "NUM_BATCHES = int(4e5)\n",
    "BATCH_SIZE = 128\n",
    "GENERATE_EVERY  = 10000\n",
    "NUM_TOKENS = 10 + 2\n",
    "ENC_SEQ_LEN = 24\n",
    "DEC_SEQ_LEN = 48\n",
    "\n",
    "INPUT_LEN = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87cb25",
   "metadata": {},
   "source": [
    "#### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b7097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "# class copy_generator:\n",
    "#     def __init__(self):\n",
    "#         self.src_mask = torch.ones(BATCH_SIZE, ENC_SEQ_LEN).bool().cuda()\n",
    "#         self.tgt_mask = torch.ones(BATCH_SIZE, DEC_SEQ_LEN+1).bool().cuda()\n",
    "    \n",
    "#     def __next__(self):\n",
    "#         X = np.zeros([BATCH_SIZE, ENC_SEQ_LEN]).astype(int)\n",
    "#         y = np.zeros([BATCH_SIZE, DEC_SEQ_LEN+1]).astype(int)\n",
    "#         y[:, 0] = 1\n",
    "#         for i in range(BATCH_SIZE):\n",
    "#             sequence_length = ENC_SEQ_LEN\n",
    "#             random_sequence = np.random.randint(2, NUM_TOKENS, sequence_length)\n",
    "            \n",
    "#             X[i, :sequence_length] = random_sequence\n",
    "#             y[i, 1: 2 * sequence_length + 1] = np.concatenate([random_sequence] * 2)\n",
    "\n",
    "#         return torch.tensor(X), torch.tensor(y), self.src_mask, self.tgt_mask \n",
    "    \n",
    "# generator = copy_generator()\n",
    "# generate_data(generator, path=f'data{INPUT_LEN}', task_name=TASK_NAME, train_size=TRAIN_SIZE, test_size=TEST_SIZE, val_size=VAL_SIZE, batch_size=BATCH_SIZE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c118128",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e975a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs:  12\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0007\n",
    "\n",
    "model_parameters = ParameterGrid({'dim': [128],\n",
    "    'tie_token_embeds': [True],\n",
    "    'return_tgt_loss': [True],\n",
    "    'enc_num_tokens': [NUM_TOKENS],\n",
    "    'depth,heads': [(2, 4)],\n",
    "    'enc_max_seq_len': [24],\n",
    "    'dec_num_tokens': [NUM_TOKENS],\n",
    "    'dec_max_seq_len': [DEC_SEQ_LEN],\n",
    "    'enc_num_memory_tokens': [2, 8, 0]})\n",
    "\n",
    "print('Total runs: ', NUM_INITS * len(model_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92978633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: "
     ]
    }
   ],
   "source": [
    "gen_train = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_train', batch_size=BATCH_SIZE)\n",
    "gen_val = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_val', batch_size=VAL_SIZE)\n",
    "gen_test = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_test', batch_size=TEST_SIZE)\n",
    "\n",
    "\n",
    "print_file = f'logs/{TASK_NAME}_{TAG}_memory_logs2.txt'\n",
    "t = time.time()\n",
    "with torch.cuda.device(1):\n",
    "    for init_num in range(NUM_INITS):\n",
    "        with open(print_file, 'a') as f:\n",
    "            f.write('\\n\\nInit number ' + str(init_num)+'\\n')\n",
    "        for i, param in enumerate(list(model_parameters)):\n",
    "            with open(print_file, 'a') as f:\n",
    "                f.write('\\n\\n' + str(param)+'\\n')\n",
    "            param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
    "            param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
    "            param.pop('depth,heads')\n",
    "\n",
    "            with open(print_file, 'a') as f:\n",
    "                f.write(f'{i / len(model_parameters) * 100}%')\n",
    "            model = XTransformer(**param).cuda()\n",
    "\n",
    "            model_name = f\"{TASK_NAME}{INPUT_LEN}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_{TAG}_v{init_num}\"\n",
    "\n",
    "            optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            train_validate_model(model, \n",
    "                            train_generator=gen_train, \n",
    "                            val_generator=gen_val, \n",
    "                            optim=optim, \n",
    "                            model_name=model_name, \n",
    "                            config=param,\n",
    "                            num_batches=NUM_BATCHES,\n",
    "                            generate_every=GENERATE_EVERY,\n",
    "                            print_file=print_file,\n",
    "                            tag=TAG,\n",
    "                            overfit_stop=False)\n",
    "            test_model(model, gen_test, model_name, param, TASK_NAME, tag=TAG)\n",
    "            with open(print_file, 'a') as f:\n",
    "                f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
    "            t = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5d48a",
   "metadata": {},
   "source": [
    "### Refit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfb752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def load_cpt(config, v, task_name, input_length):\n",
    "#     for fns in os.walk('checkpoints'):\n",
    "#         model_names = fns[2]\n",
    "        \n",
    "#     prefix = '{task_name}_dim{dim}d{d}h{h}M{M}l{l}'\n",
    "#     name = prefix.format(task_name=task_name,\n",
    "#                         dim=config['dim'],\n",
    "#                         d=config['enc_depth'], h=config['enc_heads'], \n",
    "#                         M=config['enc_num_memory_tokens'], \n",
    "#                         l=input_length)\n",
    "\n",
    "#     checkpoint_paths = ['checkpoints/' + n for n in model_names if name in n]\n",
    "#     cpt = torch.load(checkpoint_paths[v])\n",
    "#     bn, model_state, optim_state = cpt['batch_num'], cpt['state_dict'], cpt['optimizer']\n",
    "\n",
    "#     model = XTransformer(**config).cuda()\n",
    "#     model.load_state_dict(model_state)\n",
    "\n",
    "#     optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     optim.load_state_dict(optim_state)\n",
    "\n",
    "#     return bn, model, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396ecf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAG = 'refit_to_max'\n",
    "# LEARNING_RATE = 0.001\n",
    "\n",
    "# path = f\"checkpoints/{TASK_NAME}{INPUT_LEN}/\"\n",
    "\n",
    "# for name in next(os.walk(path))[2]:\n",
    "#     print(name)\n",
    "#     if name == 'copy24_dim128d2h4M12l12_10tkn_len24_v2_10tkn_len24.pt':\n",
    "#         continue\n",
    "#     cpt = torch.load(path+name)\n",
    "#     print(cpt['batch_num'])\n",
    "#     delta_batches = NUM_BATCHES - cpt['batch_num'] - 1\n",
    "#     if delta_batches < 1:\n",
    "#         continue\n",
    "    \n",
    "#     split = name.split('_')\n",
    "#     config = {'dec_max_seq_len': DEC_SEQ_LEN,\n",
    "#          'dec_num_tokens': NUM_TOKENS,\n",
    "#          'dim': int(split[1].split('dim')[1].split('d')[0]),\n",
    "#          'enc_max_seq_len': int(split[1].split('M')[1].split('l')[1]),\n",
    "#          'enc_num_memory_tokens': int(split[1].split('M')[1].split('l')[0]),\n",
    "#          'enc_num_tokens': NUM_TOKENS,\n",
    "#          'return_tgt_loss': True,\n",
    "#          'tie_token_embeds': True,\n",
    "#          'enc_depth': int(split[1][3:].split('d')[1].split('h')[0]),\n",
    "#          'enc_heads': int(split[1][3:].split('d')[1].split('h')[1].split('M')[0]),\n",
    "#          'dec_depth': int(split[1][3:].split('d')[1].split('h')[0]),\n",
    "#          'dec_heads': int(split[1][3:].split('d')[1].split('h')[1].split('M')[0]),\n",
    "#          'tag': TAG,\n",
    "#          'task_name': TASK_NAME}\n",
    "    \n",
    "    \n",
    "#     gen_train = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_train', batch_size=BATCH_SIZE)\n",
    "#     gen_val = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_val', batch_size=VAL_SIZE)\n",
    "#     gen_test = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_test', batch_size=TEST_SIZE)\n",
    "\n",
    "\n",
    "#     print_file = f'logs/{TASK_NAME}_{TAG}_memory_logs.txt'\n",
    "#     t = time.time()\n",
    "#     with torch.cuda.device(0):\n",
    "#         with open(print_file, 'a') as f:\n",
    "#             f.write('\\n\\n' + str(config)+'\\n')\n",
    "#             f.write(str(delta_batches) + ' batches to go.\\n')\n",
    "\n",
    "#         print('\\n\\n' + str(config)+'\\n')\n",
    "#         print(str(delta_batches) + ' batches to go.\\n')\n",
    "#         model_name = name\n",
    "#         model = XTransformer(**config).cuda()\n",
    "#         optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "#         model.load_state_dict(cpt['state_dict'])\n",
    "#         optim.load_state_dict(cpt['optimizer'])\n",
    "\n",
    "#         train_validate_model(model, \n",
    "#                             train_generator=gen_train, \n",
    "#                             val_generator=gen_val, \n",
    "#                             optim=optim, \n",
    "#                             model_name=model_name, \n",
    "#                             config=config,\n",
    "#                             num_batches=delta_batches,\n",
    "#                             generate_every=GENERATE_EVERY,\n",
    "#                             print_file=print_file,\n",
    "#                             tag=TAG,\n",
    "#                             overfit_stop=False)\n",
    "#         test_model(model, gen_test, model_name, config, TASK_NAME, tag=TAG)\n",
    "\n",
    "#         with open(print_file, 'a') as f:\n",
    "#             f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
    "#         t = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, gen_test, model_name, config, TASK_NAME, tag=TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_train = data_loader(task_name=f'{TASK_NAME}_train', batch_size=BATCH_SIZE, enc_seq_len=INPUT_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
    "# gen_val = data_loader(task_name=f'{TASK_NAME}_val', batch_size=VAL_SIZE, enc_seq_len=INPUT_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
    "# gen_test = data_loader(task_name=f'{TASK_NAME}_test', batch_size=TEST_SIZE, enc_seq_len=INPUT_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
    "\n",
    "\n",
    "# print_file = f'logs/{TASK_NAME}_{TAG}_memory_logs.txt'\n",
    "# t = time.time()\n",
    "# with torch.cuda.device(0):\n",
    "#     for init_num in range(NUM_INITS):\n",
    "#         with open(print_file, 'a') as f:\n",
    "#             f.write('\\n\\nInit number ' + str(init_num)+'\\n')\n",
    "#         for i, param in enumerate(list(model_parameters)):\n",
    "#             with open(print_file, 'a') as f:\n",
    "#                 f.write('\\n\\n' + str(param)+'\\n')\n",
    "#             param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
    "#             param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
    "#             param.pop('depth,heads')\n",
    "\n",
    "#             with open(print_file, 'a') as f:\n",
    "#                 f.write(f'{i / len(model_parameters) * 100}%')\n",
    "#             model = XTransformer(**param).cuda()\n",
    "\n",
    "#             model_name = f\"{TASK_NAME}{INPUT_LEN}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_v{init_num}\"\n",
    "\n",
    "#             optim = torch.optim.Adam(model.ффparameters(), lr=LEARNING_RATE)\n",
    "            \n",
    "#             bn, model, optim = load_cpt(param, v=init_num, task_name='copy55', input_length=param['enc_max_seq_len'])\n",
    "#             with open(print_file, 'a') as f:\n",
    "#                 f.write(f'BN: {bn}\\n')\n",
    "#             if bn < 130_000:\n",
    "#                 train_validate_model(model, \n",
    "#                                     train_generator=gen_train, \n",
    "#                                     val_generator=gen_val, \n",
    "#                                     optim=optim, \n",
    "#                                     model_name=model_name, \n",
    "#                                     dec_seq_len=DEC_SEQ_LEN,\n",
    "#                                     num_batches=NUM_BATCHES,\n",
    "#                                     generate_every=GENERATE_EVERY,\n",
    "#                                     print_file=print_file,\n",
    "#                                     tag=TAG,\n",
    "#                                     overfit_stop=False,\n",
    "#                                     head_start=(130_000 - bn)/GENERATE_EVERY)\n",
    "#                 test_model(model, gen_test, model_name, param, TASK_NAME, tag=TAG, dec_seq_len=param['dec_max_seq_len'])\n",
    "#             with open(print_file, 'a') as f:\n",
    "#                 f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
    "#             t = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c92dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_experiment import save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e649122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f'checkpoints/{model_name}_b{i}_{TAG}_maxval.pt'\n",
    "# save_cpt(save_path, model, optim)\n",
    "\n",
    "# if i // generate_every < head_start:\n",
    "#     continue\n",
    "\n",
    "# # early stopping\n",
    "# smoothed_val_scores = [np.mean(validation_scores[i-WINDOW_SIZE+1:i]) for i in range(WINDOW_SIZE-1, len(validation_scores))]\n",
    "\n",
    "# if overfit_stop and max(smoothed_val_scores) > max(smoothed_val_scores[-PATIENCE:]):\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fca13",
   "metadata": {},
   "source": [
    "### Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09847b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_num = 0\n",
    "\n",
    "gen_train = data_loader(task_name=f'{TASK_NAME}_train', batch_size=BATCH_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
    "gen_val = data_loader(task_name=f'{TASK_NAME}_val', batch_size=VAL_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
    "gen_test = data_loader(task_name=f'{TASK_NAME}_test', batch_size=TEST_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
    "\n",
    "\n",
    "param = list(model_parameters)[5]\n",
    "print(param)\n",
    "param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
    "param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
    "param.pop('depth,heads')\n",
    "\n",
    "model = XTransformer(**param).cuda()\n",
    "\n",
    "model_name = f\"{TASK_NAME}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_v{init_num}\"\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "src, tgt, _, _ = next(gen_train)\n",
    "\n",
    "print(model.encoder.max_seq_len, model.encoder.num_memory_tokens)\n",
    "model.encoder(torch.cat((src, src)), return_embeddings=True).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
