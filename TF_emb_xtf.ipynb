{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1bcb7ac1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bcb7ac1",
        "outputId": "8d996800-e0a3-456b-a512-87d834773c48"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/booydar/algotrade\n",
        "# !pip install einops entmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aQ1-2CT-_dOM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ1-2CT-_dOM",
        "outputId": "528ea27e-8996-44cc-e838-fb0b5c6a7d83"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "46293b71",
      "metadata": {
        "id": "46293b71"
      },
      "outputs": [],
      "source": [
        "# !mkdir logs\n",
        "# !mkdir checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b9dfb631",
      "metadata": {
        "id": "b9dfb631"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# sys.path.append('algotrade/NN')\n",
        "# from algotrade.NN.x_transformers.x_transformers import *\n",
        "# from algotrade.NN.run_experiment import *\n",
        "# from algotrade.NN.generate_data import *\n",
        "\n",
        "sys.path.append('NN')\n",
        "from x_transformers.NN.x_transformers import *\n",
        "from x_transformers.NN.x_transformers.x_transformers import *\n",
        "from x_transformers.NN.run_experiment import *\n",
        "# from x_transformers.NN.generate_data import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746e6986",
      "metadata": {
        "id": "746e6986"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1de49706",
      "metadata": {
        "id": "1de49706"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "TAG = 'test'\n",
        "\n",
        "TASK_NAME = 'price'\n",
        "TRAIN_SIZE = 100_000\n",
        "VAL_SIZE = 2_000\n",
        "TEST_SIZE = 10_000\n",
        "NUM_INITS = 4\n",
        "\n",
        "\n",
        "NUM_BATCHES = int(4e5)\n",
        "BATCH_SIZE = 128\n",
        "GENERATE_EVERY  = 10000\n",
        "NUM_TOKENS = 10 + 2\n",
        "ENC_SEQ_LEN = 24\n",
        "DEC_SEQ_LEN = 48\n",
        "\n",
        "INPUT_LEN = 24"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a87cb25",
      "metadata": {
        "id": "7a87cb25"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ZefqWL3W7WkY",
      "metadata": {
        "id": "ZefqWL3W7WkY"
      },
      "outputs": [],
      "source": [
        "class data_loader:\n",
        "    def __init__(self, mode, path='data', tgt_len=24, batch_size=32, tgt_dim=2, device='cpu'):\n",
        "        X, y = np.load(f'{path}/X_{mode}.npy'), np.load(f'{path}/y_{mode}.npy')        \n",
        "        X = torch.tensor(X)\n",
        "\n",
        "        slices_x = [X[i:tgt_len + i] for i in range(X.shape[0] - tgt_len)]\n",
        "        src = torch.stack(slices_x)\n",
        "        tgt = y[tgt_len-1:-1]\n",
        "        \n",
        "        if tgt_dim is not None:\n",
        "            tgt = tgt[:, [0, tgt_dim]]\n",
        "        \n",
        "        perm_ind = torch.randperm(src.shape[0])\n",
        "        src, tgt = src[perm_ind], tgt[perm_ind]\n",
        "        self.src, self.tgt = torch.tensor(src).float(), torch.tensor(tgt).float()\n",
        "\n",
        "        self.data_size = self.src.shape[0]\n",
        "        self.data_ptr = 0\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.data_ptr + self.batch_size > self.data_size:\n",
        "            self.data_ptr = 0\n",
        "\n",
        "        src = self.src[self.data_ptr: self.data_ptr + self.batch_size].to(device=self.device)\n",
        "        tgt = self.tgt[self.data_ptr: self.data_ptr + self.batch_size].to(device=self.device)\n",
        "        \n",
        "        src_mask = tgt_mask = None\n",
        "            \n",
        "        self.data_ptr = (self.data_ptr + self.batch_size) % self.data_size\n",
        "\n",
        "        return src, tgt, src_mask, tgt_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c118128",
      "metadata": {
        "id": "1c118128"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ibxQ2_rDA6cZ",
      "metadata": {
        "id": "ibxQ2_rDA6cZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-ef7a558fc6f6>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.src, self.tgt = torch.tensor(src).float(), torch.tensor(tgt).float()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gen_train = data_loader(path=f'data/BTCUSD', mode='train', batch_size=BATCH_SIZE, device='cuda')\n",
        "gen_val = data_loader(path=f'data/BTCUSD', mode='val', batch_size=BATCH_SIZE, device='cuda')\n",
        "gen_test = data_loader(path=f'data/BTCUSD', mode='test', batch_size=BATCH_SIZE, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "KIiuA3v357ou",
      "metadata": {
        "id": "KIiuA3v357ou"
      },
      "outputs": [],
      "source": [
        "class CXTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        tie_token_emb = False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        enc_kwargs, kwargs = groupby_prefix_and_trim('enc_', kwargs)\n",
        "        dec_kwargs, kwargs = groupby_prefix_and_trim('dec_', kwargs)\n",
        "        \n",
        "        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs, 'dimension of either encoder or decoder must be set with `dim` keyword'\n",
        "        enc_transformer_kwargs = pick_and_pop(['max_seq_len', 'dim_in', 'use_pos_emb'], enc_kwargs)\n",
        "        # enc_transformer_kwargs['num_memory_tokens'] = enc_kwargs.pop('num_memory_tokens', None)\n",
        "\n",
        "        dec_transformer_kwargs = pick_and_pop(['max_seq_len', 'dim_in', 'dim_out'], dec_kwargs)\n",
        "\n",
        "        self.encoder = ContinuousTransformerWrapper(\n",
        "            **enc_transformer_kwargs,\n",
        "            attn_layers = Encoder(dim = dim, **enc_kwargs)\n",
        "        )\n",
        "\n",
        "        self.decoder = ContinuousTransformerWrapper(\n",
        "            **dec_transformer_kwargs,\n",
        "            attn_layers = Decoder(dim = dim, cross_attend = True, **dec_kwargs)\n",
        "        )\n",
        "\n",
        "        if tie_token_emb:\n",
        "            self.decoder.token_emb = self.encoder.token_emb\n",
        "\n",
        "        self.decoder = AutoregressiveWrapper(self.decoder)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, seq_in, seq_out_start, seq_len, src_mask = None, **kwargs):\n",
        "        encodings = self.encoder(seq_in, return_embeddings = True, mask = src_mask)\n",
        "        return self.decoder.generate(seq_out_start, seq_len, context = encodings, context_mask = src_mask, **kwargs)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask = None, tgt_mask = None):\n",
        "        enc = model.encoder(src, mask = src_mask, return_embeddings = True)\n",
        "    \n",
        "        gen_token = -10_000 * torch.ones_like(src[:, :1, :])\n",
        "\n",
        "        out = model.decoder.net(gen_token, context=enc)\n",
        "        xo = tgt[:, 1:]\n",
        "        loss = F.mse_loss(out.transpose(1, 2)[:, 0], xo)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "jTkwLzz75eLp",
      "metadata": {
        "id": "jTkwLzz75eLp"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "model_parameters = ParameterGrid({'dim': [128],\n",
        "    'tie_token_embeds': [True],\n",
        "    'return_tgt_loss': [True],\n",
        "    'enc_depth': [2],\n",
        "    'enc_heads': [4],\n",
        "    'dec_depth': [2],\n",
        "    'dec_heads': [4],\n",
        "    'enc_max_seq_len': [24],\n",
        "    'dec_max_seq_len': [1],\n",
        "    'enc_num_memory_tokens': [0],\n",
        "    'enc_dim_in': [16],\n",
        "    'dec_dim_in': [16],\n",
        "    'enc_dim_out': [1],\n",
        "    'dec_dim_out': [1],\n",
        "    'enc_emb_dim': [128],\n",
        "    'enc_emb_dropout': [0.],\n",
        "    'enc_use_pos_emb': [False]\n",
        "})\n",
        "\n",
        "param = list(model_parameters)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "e24b7cae",
      "metadata": {},
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 4\n",
        "PATIENCE = 10\n",
        "def train_validate_model(model, train_generator, val_generator, optim, model_name, config, generate_every=1e2, num_batches=1e3, verbose=True, overfit_stop=True, print_file=None, tag='', log_path='logs/', head_start=15):\n",
        "    \n",
        "    t0 = time.time()\n",
        "    \n",
        "    log_dir = log_path + model_name.split('_')[0]\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    if print_file is None:\n",
        "        print_file = f\"{log_dir}/{model_name}_cout_log.txt\"\n",
        "\n",
        "    validation_scores = []\n",
        "    for i in range(num_batches):\n",
        "\n",
        "        model.train()\n",
        "        \n",
        "        src, tgt, src_mask, tgt_mask = next(train_generator)\n",
        "        loss = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "        loss.backward()\n",
        "\n",
        "        loss_value = loss.item()        \n",
        "        writer.add_scalars(\"/train/loss\", {model_name: loss_value}, i)\n",
        "#         if loss_value < 1e-10:\n",
        "#             break\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "        if i != 0 and i % generate_every == 0:\n",
        "            model.eval()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                src, tgt, src_mask, tgt_mask = next(val_generator)\n",
        "                \n",
        "                enc = model.encoder(src, mask = src_mask, return_embeddings = True)\n",
        "    \n",
        "                gen_token = -10_000 * torch.ones_like(src[:, :1, :])\n",
        "\n",
        "                out = model.decoder.net(gen_token, context=enc)\n",
        "                xo = tgt[:, 1:]\n",
        "                val_loss = F.mse_loss(out.transpose(1, 2)[:, 0], xo)\n",
        "                val_loss_value = val_loss.item()\n",
        "\n",
        "            writer.add_scalars(\"/val/loss\", {model_name: val_loss_value}, i)\n",
        "\n",
        "            validation_scores.append(val_loss_value) \n",
        "    \n",
        "            if verbose:\n",
        "                with open(print_file, 'a') as f:\n",
        "                    f.write(f\"\\n\\ninput:  {src[0][-5:, 0]}\")\n",
        "                    f.write(f\"\\npredicted output:  {out[0]}\")\n",
        "                    f.write(f\"\\ncorrect output:  {xo[0]}\")\n",
        "                    f.write(f\"\\ntime: {round(time.time() - t0)}\")\n",
        "                    t0 = time.time()\n",
        "            \n",
        "            # save checkpoint\n",
        "            if max(validation_scores) == validation_scores[-1]:\n",
        "                os.system(f'mkdir {log_path}checkpoints')\n",
        "                os.system(f'mkdir {log_path}checkpoints/{model_name.split(\"_\")[0]}')\n",
        "                os.system(f'mkdir {log_path}checkpoints/{model_name.split(\"_\")[0]}/validation')\n",
        "                save_path = f'{log_path}checkpoints/{model_name.split(\"_\")[0]}/validation/{model_name}_{tag}_maxval.pt'\n",
        "                save_checkpoint(save_path, model, optim, i, config)\n",
        "                \n",
        "            if i // generate_every < head_start:\n",
        "                continue\n",
        "                \n",
        "            # early stopping\n",
        "            smoothed_val_scores = [np.mean(validation_scores[i-WINDOW_SIZE+1:i]) for i in range(WINDOW_SIZE-1, len(validation_scores))]\n",
        "            \n",
        "            if overfit_stop and max(smoothed_val_scores) > max(smoothed_val_scores[-PATIENCE:]):\n",
        "                break\n",
        "                \n",
        "    # save checkpoint\n",
        "    save_path = f'{log_path}checkpoints/{model_name.split(\"_\")[0]}/{model_name}_{tag}.pt'\n",
        "    os.system(f'mkdir {log_path}checkpoints/{model_name.split(\"_\")[0]}')\n",
        "    save_checkpoint(save_path, model, optim, i, config)\n",
        "\n",
        "    writer.flush()\n",
        "\n",
        "\n",
        "def test_model(model, test_generator, model_name, param, task_name, tag, num_batches=50, log_path='logs/_test_results.csv'):\n",
        "    model.eval()\n",
        "\n",
        "    loss_values = []\n",
        "    with torch.no_grad():\n",
        "        for bn in range(num_batches):\n",
        "            src, tgt, src_mask, tgt_mask = next(test_generator)\n",
        "            \n",
        "            enc = model.encoder(src, mask = src_mask, return_embeddings = True)\n",
        "\n",
        "            gen_token = -10_000 * torch.ones_like(src[:, :1, :])\n",
        "\n",
        "            out = model.decoder.net(gen_token, context=enc)\n",
        "            xo = tgt[:, 1:]\n",
        "            loss = F.mse_loss(out.transpose(1, 2)[:, 0], xo)\n",
        "            loss_values.append(loss.cpu().item())\n",
        "\n",
        "    param['tag'] = tag\n",
        "    param['task_name'] = task_name\n",
        "    param['model_name'] = model_name\n",
        "    param['loss'] = np.mean(loss_values)\n",
        "\n",
        "    if os.path.exists(log_path):\n",
        "        df = pd.read_csv(log_path)\n",
        "        df = df.append(param, ignore_index=True)\n",
        "    else: \n",
        "        df = pd.DataFrame([param])\n",
        "    df.to_csv(log_path, index=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "936e7d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "GENERATE_EVERY = 100\n",
        "NUM_BATCHES = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "333d0f28",
      "metadata": {},
      "outputs": [],
      "source": [
        "drive_path = 'stocks_logs/'\n",
        "print_file = f'{drive_path}{TAG}_logs.txt'\n",
        "t = time.time()\n",
        "for init_num in range(1):\n",
        "    with open(print_file, 'a') as f:\n",
        "        f.write('\\n\\nInit number ' + str(init_num)+'\\n')\n",
        "    for i, param in enumerate(list(model_parameters)):\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write('\\n\\n' + str(param)+'\\n')\n",
        "        # param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
        "        # param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
        "        # param.pop('depth,heads')\n",
        "\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write(f'{i / len(model_parameters) * 100}%')\n",
        "        model = CXTransformer(**param).cuda()\n",
        "\n",
        "        model_name = f\"{TASK_NAME}{INPUT_LEN}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_{TAG}_v{init_num}\"\n",
        "\n",
        "        optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "        train_validate_model(model, \n",
        "                        train_generator=gen_train, \n",
        "                        val_generator=gen_val, \n",
        "                        optim=optim, \n",
        "                        model_name=model_name, \n",
        "                        config=param,\n",
        "                        num_batches=NUM_BATCHES,\n",
        "                        generate_every=GENERATE_EVERY,\n",
        "                        print_file=print_file,\n",
        "                        tag=TAG,\n",
        "                        overfit_stop=False)\n",
        "        test_model(model, gen_test, model_name, param, TASK_NAME, tag=TAG, log_path=drive_path+'test_results.csv')\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
        "        t = time.time()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TF_emb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
