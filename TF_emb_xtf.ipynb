{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1bcb7ac1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bcb7ac1",
        "outputId": "8d996800-e0a3-456b-a512-87d834773c48"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/booydar/algotrade\n",
        "# !pip install einops entmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aQ1-2CT-_dOM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ1-2CT-_dOM",
        "outputId": "528ea27e-8996-44cc-e838-fb0b5c6a7d83"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "46293b71",
      "metadata": {
        "id": "46293b71"
      },
      "outputs": [],
      "source": [
        "# !mkdir logs\n",
        "# !mkdir checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b9dfb631",
      "metadata": {
        "id": "b9dfb631"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# sys.path.append('algotrade/NN')\n",
        "# from algotrade.NN.x_transformers.x_transformers import *\n",
        "# from algotrade.NN.run_experiment import *\n",
        "# from algotrade.NN.generate_data import *\n",
        "\n",
        "sys.path.append('NN')\n",
        "from x_transformers.NN.x_transformers import *\n",
        "from x_transformers.NN.x_transformers.x_transformers import *\n",
        "from x_transformers.NN.run_experiment import *\n",
        "# from x_transformers.NN.generate_data import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746e6986",
      "metadata": {
        "id": "746e6986"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1de49706",
      "metadata": {
        "id": "1de49706"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "TAG = 'test'\n",
        "\n",
        "TASK_NAME = 'price'\n",
        "TRAIN_SIZE = 100_000\n",
        "VAL_SIZE = 2_000\n",
        "TEST_SIZE = 10_000\n",
        "NUM_INITS = 4\n",
        "\n",
        "\n",
        "NUM_BATCHES = int(4e5)\n",
        "BATCH_SIZE = 128\n",
        "GENERATE_EVERY  = 10000\n",
        "NUM_TOKENS = 10 + 2\n",
        "ENC_SEQ_LEN = 24\n",
        "DEC_SEQ_LEN = 48\n",
        "\n",
        "INPUT_LEN = 24"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a87cb25",
      "metadata": {
        "id": "7a87cb25"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ZefqWL3W7WkY",
      "metadata": {
        "id": "ZefqWL3W7WkY"
      },
      "outputs": [],
      "source": [
        "class data_loader:\n",
        "    def __init__(self, mode, path='data', tgt_len=24, batch_size=32, tgt_dim=2, device='cpu'):\n",
        "        X, y = np.load(f'{path}/X_{mode}.npy'), np.load(f'{path}/y_{mode}.npy')        \n",
        "        X = torch.tensor(X)\n",
        "\n",
        "        slices_x = [X[i:tgt_len + i] for i in range(X.shape[0] - tgt_len)]\n",
        "        src = torch.stack(slices_x)\n",
        "        tgt = y[tgt_len-1:-1]\n",
        "        \n",
        "        if tgt_dim is not None:\n",
        "            tgt = tgt[:, [0, tgt_dim]]\n",
        "        \n",
        "        perm_ind = torch.randperm(src.shape[0])\n",
        "        src, tgt = src[perm_ind], tgt[perm_ind]\n",
        "        self.src, self.tgt = torch.tensor(src).float(), torch.tensor(tgt).float()\n",
        "\n",
        "        self.data_size = self.src.shape[0]\n",
        "        self.data_ptr = 0\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.data_ptr + self.batch_size > self.data_size:\n",
        "            self.data_ptr = 0\n",
        "\n",
        "        src = self.src[self.data_ptr: self.data_ptr + self.batch_size].to(device=self.device)\n",
        "        tgt = self.tgt[self.data_ptr: self.data_ptr + self.batch_size].to(device=self.device)\n",
        "        \n",
        "        src_mask = tgt_mask = None\n",
        "            \n",
        "        self.data_ptr = (self.data_ptr + self.batch_size) % self.data_size\n",
        "\n",
        "        return src, tgt, src_mask, tgt_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c118128",
      "metadata": {
        "id": "1c118128"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ibxQ2_rDA6cZ",
      "metadata": {
        "id": "ibxQ2_rDA6cZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-ef7a558fc6f6>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.src, self.tgt = torch.tensor(src).float(), torch.tensor(tgt).float()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gen_train = data_loader(path=f'data/BTCUSD', mode='train', batch_size=BATCH_SIZE, device='cuda')\n",
        "gen_val = data_loader(path=f'data/BTCUSD', mode='val', batch_size=BATCH_SIZE, device='cuda')\n",
        "gen_test = data_loader(path=f'data/BTCUSD', mode='test', batch_size=BATCH_SIZE, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "KIiuA3v357ou",
      "metadata": {
        "id": "KIiuA3v357ou"
      },
      "outputs": [],
      "source": [
        "class CXTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        tie_token_emb = False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        enc_kwargs, kwargs = groupby_prefix_and_trim('enc_', kwargs)\n",
        "        dec_kwargs, kwargs = groupby_prefix_and_trim('dec_', kwargs)\n",
        "        \n",
        "        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs, 'dimension of either encoder or decoder must be set with `dim` keyword'\n",
        "        enc_transformer_kwargs = pick_and_pop(['max_seq_len', 'dim_in', 'use_pos_emb'], enc_kwargs)\n",
        "        # enc_transformer_kwargs['num_memory_tokens'] = enc_kwargs.pop('num_memory_tokens', None)\n",
        "\n",
        "        dec_transformer_kwargs = pick_and_pop(['max_seq_len', 'dim_in', 'dim_out'], dec_kwargs)\n",
        "\n",
        "        self.encoder = ContinuousTransformerWrapper(\n",
        "            **enc_transformer_kwargs,\n",
        "            attn_layers = Encoder(dim = dim, **enc_kwargs)\n",
        "        )\n",
        "\n",
        "        self.decoder = ContinuousTransformerWrapper(\n",
        "            **dec_transformer_kwargs,\n",
        "            attn_layers = Decoder(dim = dim, cross_attend = True, **dec_kwargs)\n",
        "        )\n",
        "\n",
        "        if tie_token_emb:\n",
        "            self.decoder.token_emb = self.encoder.token_emb\n",
        "\n",
        "        self.decoder = AutoregressiveWrapper(self.decoder)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, seq_in, seq_out_start, seq_len, src_mask = None, **kwargs):\n",
        "        encodings = self.encoder(seq_in, return_embeddings = True, mask = src_mask)\n",
        "        return self.decoder.generate(seq_out_start, seq_len, context = encodings, context_mask = src_mask, **kwargs)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask = None, tgt_mask = None):\n",
        "        enc = model.encoder(src, mask = src_mask, return_embeddings = True)\n",
        "    \n",
        "        gen_token = -10_000 * torch.ones_like(src[:, :1, :])\n",
        "\n",
        "        out = model.decoder.net(gen_token, context=enc)\n",
        "        xo = tgt[:, 1:]\n",
        "        loss = F.mse_loss(out.transpose(1, 2)[:, 0], xo)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "jTkwLzz75eLp",
      "metadata": {
        "id": "jTkwLzz75eLp"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "model_parameters = ParameterGrid({'dim': [128],\n",
        "    'tie_token_embeds': [True],\n",
        "    'return_tgt_loss': [True],\n",
        "    'enc_depth': [2],\n",
        "    'enc_heads': [4],\n",
        "    'dec_depth': [2],\n",
        "    'dec_heads': [4],\n",
        "    'enc_max_seq_len': [24],\n",
        "    'dec_max_seq_len': [1],\n",
        "    'enc_num_memory_tokens': [0],\n",
        "    'enc_dim_in': [16],\n",
        "    'dec_dim_in': [16],\n",
        "    'enc_dim_out': [1],\n",
        "    'dec_dim_out': [1],\n",
        "    'enc_emb_dim': [128],\n",
        "    'enc_emb_dropout': [0.],\n",
        "    'enc_use_pos_emb': [False]\n",
        "})\n",
        "\n",
        "param = list(model_parameters)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "e24b7cae",
      "metadata": {},
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 4\n",
        "PATIENCE = 10\n",
        "def train_validate_model(model, train_generator, val_generator, optim, model_name, config, generate_every=1e2, num_batches=1e3, verbose=True, overfit_stop=True, print_file=None, tag='', log_path='logs/', head_start=15):\n",
        "    \n",
        "    fix_seeds()\n",
        "    t0 = time.time()\n",
        "    \n",
        "    log_dir = log_path + model_name.split('_')[0]\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    if print_file is None:\n",
        "        print_file = f\"{log_dir}/{model_name}_cout_log.txt\"\n",
        "\n",
        "    validation_scores = []\n",
        "    for i in range(num_batches):\n",
        "\n",
        "        model.train()\n",
        "        \n",
        "        src, tgt, src_mask, tgt_mask = next(train_generator)\n",
        "        loss = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "        loss.backward()\n",
        "\n",
        "        loss_value = loss.item()        \n",
        "        writer.add_scalars(\"/train/loss\", {model_name: loss_value}, i)\n",
        "#         if loss_value < 1e-10:\n",
        "#             break\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "        if i != 0 and i % generate_every == 0:\n",
        "            model.eval()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                src, tgt, src_mask, tgt_mask = next(val_generator)\n",
        "                \n",
        "                enc = model.encoder(src, mask = src_mask, return_embeddings = True)\n",
        "    \n",
        "                gen_token = -10_000 * torch.ones_like(src[:, :1, :])\n",
        "\n",
        "                out = model.decoder.net(gen_token, context=enc)\n",
        "                xo = tgt[:, 1:]\n",
        "                val_loss = F.mse_loss(out.transpose(1, 2)[:, 0], xo)\n",
        "                val_loss_value = val_loss.item()\n",
        "\n",
        "            writer.add_scalars(\"/val/loss\", {model_name: val_loss_value}, i)\n",
        "\n",
        "            validation_scores.append(val_loss_value) \n",
        "    \n",
        "            if verbose:\n",
        "                with open(print_file, 'a') as f:\n",
        "                    f.write(f\"\\n\\ninput:  {src[0]}\")\n",
        "                    f.write(f\"\\npredicted output:  {out[0]}\")\n",
        "                    f.write(f\"\\ncorrect output:  {xo[0]}\")\n",
        "                    f.write(f\"\\ntime: {round(time.time() - t0)}\")\n",
        "                    t0 = time.time()\n",
        "            \n",
        "            # save checkpoint\n",
        "            if max(validation_scores) == validation_scores[-1]:\n",
        "                os.system(f'mkdir {log_path}checkpoints')\n",
        "                os.system(f'mkdir {log_path}checkpoints/{model_name.split(\"_\")[0]}')\n",
        "                os.system(f'mkdir {log_path}checkpoints/{model_name.split(\"_\")[0]}/validation')\n",
        "                save_path = f'{log_path}checkpoints/{model_name.split(\"_\")[0]}/validation/{model_name}_{tag}_maxval.pt'\n",
        "                save_checkpoint(save_path, model, optim, i, config)\n",
        "                \n",
        "            if i // generate_every < head_start:\n",
        "                continue\n",
        "                \n",
        "            # early stopping\n",
        "            smoothed_val_scores = [np.mean(validation_scores[i-WINDOW_SIZE+1:i]) for i in range(WINDOW_SIZE-1, len(validation_scores))]\n",
        "            \n",
        "            if overfit_stop and max(smoothed_val_scores) > max(smoothed_val_scores[-PATIENCE:]):\n",
        "                break\n",
        "                \n",
        "    # save checkpoint\n",
        "    save_path = f'{log_path}checkpoints/{model_name.split(\"_\")[0]}/{model_name}_{tag}.pt'\n",
        "    os.system(f'mkdir {log_path}checkpoints/{model_name.split(\"_\")[0]}')\n",
        "    save_checkpoint(save_path, model, optim, i, config)\n",
        "\n",
        "    writer.flush()\n",
        "\n",
        "\n",
        "def test_model(model, test_generator, model_name, param, task_name, tag, num_batches=50, log_path='logs/_test_results.csv'):\n",
        "    fix_seeds()\n",
        "    model.eval()\n",
        "\n",
        "    loss_values = []\n",
        "    with torch.no_grad():\n",
        "        for bn in range(num_batches):\n",
        "            src, tgt, src_mask, tgt_mask = next(test_generator)\n",
        "            \n",
        "            enc = model.encoder(src, mask = src_mask, return_embeddings = True)\n",
        "\n",
        "            gen_token = -10_000 * torch.ones_like(src[:, :1, :])\n",
        "\n",
        "            out = model.decoder.net(gen_token, context=enc)\n",
        "            xo = tgt[:, 1:]\n",
        "            loss = F.mse_loss(out.transpose(1, 2)[:, 0], xo)\n",
        "            loss_values.append(loss.cpu().item())\n",
        "\n",
        "    param['tag'] = tag\n",
        "    param['task_name'] = task_name\n",
        "    param['model_name'] = model_name\n",
        "    param['loss'] = np.mean(loss_values)\n",
        "\n",
        "    if os.path.exists(log_path):\n",
        "        df = pd.read_csv(log_path)\n",
        "        df = df.append(param, ignore_index=True)\n",
        "    else: \n",
        "        df = pd.DataFrame([param])\n",
        "    df.to_csv(log_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "936e7d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "GENERATE_EVERY = 100\n",
        "NUM_BATCHES = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "333d0f28",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 16x128)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-ead743dd2de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTAG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                         overfit_stop=False)\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTASK_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test_results.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nTotal time: {time.time() - t}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/_projects/stocks/x_transformers/NN/run_experiment.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model, test_generator, model_name, param, task_name, tag, log_path)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mtotal_batch_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dec_max_seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mtotal_batch_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-9fa28598e6f4>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, seq_in, seq_out_start, seq_len, src_mask, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_out_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_out_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/_projects/stocks/x_transformers/NN/x_transformers/autoregressive_wrapper.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, start_tokens, seq_len, eos_token, temperature, filter_logits_fn, filter_thres, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilter_logits_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/_projects/stocks/x_transformers/NN/x_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_embeddings, mask, return_attn, mems, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 16x128)"
          ]
        }
      ],
      "source": [
        "drive_path = 'stocks_logs/'\n",
        "print_file = f'{drive_path}{TAG}_logs.txt'\n",
        "t = time.time()\n",
        "for init_num in range(1):\n",
        "    with open(print_file, 'a') as f:\n",
        "        f.write('\\n\\nInit number ' + str(init_num)+'\\n')\n",
        "    for i, param in enumerate(list(model_parameters)):\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write('\\n\\n' + str(param)+'\\n')\n",
        "        # param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
        "        # param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
        "        # param.pop('depth,heads')\n",
        "\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write(f'{i / len(model_parameters) * 100}%')\n",
        "        model = CXTransformer(**param).cuda()\n",
        "\n",
        "        model_name = f\"{TASK_NAME}{INPUT_LEN}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_{TAG}_v{init_num}\"\n",
        "\n",
        "        optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "        train_validate_model(model, \n",
        "                        train_generator=gen_train, \n",
        "                        val_generator=gen_val, \n",
        "                        optim=optim, \n",
        "                        model_name=model_name, \n",
        "                        config=param,\n",
        "                        num_batches=NUM_BATCHES,\n",
        "                        generate_every=GENERATE_EVERY,\n",
        "                        print_file=print_file,\n",
        "                        tag=TAG,\n",
        "                        overfit_stop=False)\n",
        "        test_model(model, gen_test, model_name, param, TASK_NAME, tag=TAG, log_path=drive_path+'test_results.csv')\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
        "        t = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86Am7apyTY43",
      "metadata": {
        "id": "86Am7apyTY43"
      },
      "outputs": [],
      "source": [
        "model = CXTransformer(**param)\n",
        "\n",
        "src, tgt, _, _ = next(gen_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb90a3a1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-5de64dba3a88>:46: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = F.mse_loss(out.transpose(1, 2)[:, 0], xo)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(37729472., grad_fn=<MseLossBackward0>)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(src, tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f7dc0b0f",
      "metadata": {},
      "outputs": [
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9e1622b385b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "1/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e79f8f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 24, 16]), torch.Size([128, 2]))"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src.shape, tgt.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad2e810",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 24, 16])\n",
            "torch.Size([128, 24, 128])\n",
            "torch.Size([128, 24, 128])\n",
            "torch.Size([128, 24, 128])\n",
            "torch.Size([128, 1, 16])\n",
            "torch.Size([128, 1, 128])\n",
            "torch.Size([128, 1, 128])\n",
            "torch.Size([128, 1, 128])\n"
          ]
        }
      ],
      "source": [
        "src_mask = tgt_mask = None\n",
        "context_mask = None\n",
        "\n",
        "enc = model.encoder(src, mask = src_mask, return_embeddings = True)\n",
        "    \n",
        "gen_token = -10_000 * torch.ones_like(src[:, :1, :])\n",
        "\n",
        "# out = model.decoder(gen_token, context = enc, mask = tgt_mask, context_mask = context_mask)\n",
        "\n",
        "out = model.decoder.net(gen_token, context=enc)\n",
        "xo = tgt[:, 1:]#.float()\n",
        "# out = out.float()\n",
        "# loss = F.cross_entropy(out.transpose(1, 2), xo, ignore_index = model.decoder.ignore_index)\n",
        "loss = F.mse_loss(out.transpose(1, 2)[:, 0], xo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15fcf9cd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 24, 128]), torch.Size([128, 1, 1]), torch.Size([128, 1]))"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc.shape, out.shape, tgt[:, 1:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1a4761d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 7993.5698,   574.0500, 10375.4297,  9479.9502,   391.0100,  7323.9399,\n",
              "          430.9000,   539.5100,   637.9800,   732.5400, 10895.3203,  3560.8101,\n",
              "         7687.0601,   375.9500,   675.2700,   612.2000,  8319.1504,   616.8000,\n",
              "          385.0000,  7152.7598,   671.0000,  8182.4702,  6539.1201,   734.1200,\n",
              "         6546.3101,  7902.1602,  1083.6300, 10538.2900,  6159.8901,  5420.0000,\n",
              "          447.6700,  8699.0400,  8321.9004,  8144.2500,  1000.9700,   405.8400,\n",
              "         1094.8101,   408.6900,  9504.8496,  8734.3604,  6395.7798,   654.7000,\n",
              "         7537.9800,  7794.1499,  2544.7600,  7282.2798,  1009.4400,  9120.5596,\n",
              "          253.9200,  4008.1399,  7741.4302,  9690.2402,   452.9900,   597.9600,\n",
              "         8986.9697, 17744.5000,  1183.3101,  3900.5601,   684.4200,  3585.6299,\n",
              "         8807.0098,   388.6900,  1221.9900,   272.4500,  8759.0400,  3872.4099,\n",
              "          381.7500,  1207.6300,   609.3900,   580.6700,  4188.0000,  7722.8198,\n",
              "         4587.0000,  6744.0601,  9820.0000,  7922.3599, 10149.7803,   422.0300,\n",
              "          731.8800,  2837.5400, 10198.0303,  6081.9399,  4561.1001,  9360.0000,\n",
              "         9641.9004,  4985.0000,  1175.9800, 19002.8906,  4115.4199,  1171.9900,\n",
              "         9379.3203,  7517.7002,  7115.0200,  3593.4299,   740.6500, 14221.4502,\n",
              "          608.8600,  1171.1801, 10063.7900,  6300.9800,   977.6700,  8546.0801,\n",
              "         5309.3701,   436.1900,  6533.9199,  1045.0400, 14813.5195,   588.3400,\n",
              "          419.9900,  5850.0000,   320.0000,  3231.3899,  3369.5000,  1177.7000,\n",
              "         1217.3500,  5780.0000,  9056.1602,   458.4700,   621.5300,  1194.0000,\n",
              "         8338.6201,  4436.2300,   733.4000, 10233.9199,   641.6600,  3697.4399,\n",
              "         6582.1602, 19730.0000])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tgt[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b81464",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 16])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src[:, 0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205c62b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_token = -10_000 * torch.ones_like(src[:, :1, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b380627",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 24, 16]), torch.Size([128, 1, 16]))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src.shape, gen_token.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8430f463",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=16, out_features=128, bias=True)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.decoder.net.project_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d317bdaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 16])\n",
            "torch.Size([128, 1, 128])\n",
            "torch.Size([128, 1, 128])\n",
            "torch.Size([128, 1, 128])\n"
          ]
        }
      ],
      "source": [
        "out = model.decoder.net(gen_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5073c618",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'CXTransformer' object has no attribute 'net'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a9f619ae528b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# xo = x[:, 1:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mxo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CXTransformer' object has no attribute 'net'"
          ]
        }
      ],
      "source": [
        "# xi = x[:, :-1]\n",
        "# xo = x[:, 1:]\n",
        "\n",
        "# out = model.net(gen_token)\n",
        "xo = tgt[:, 1:]\n",
        "loss = F.cross_entropy(out.transpose(1, 2), xo, ignore_index = model.ignore_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871e0e62",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 24, 128])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1717ea50",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 24, 128])\n",
            "torch.Size([128, 24, 128])\n",
            "torch.Size([128, 24, 128])\n",
            "torch.Size([128, 1])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "output with shape [128, 1] doesn't match the broadcast shape [1, 128, 128]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2520061434df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-2592eedc2937>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mcontext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/_projects/stocks/x_transformers/NN/x_transformers/autoregressive_wrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Desktop/_projects/stocks/x_transformers/NN/x_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_embeddings, mask, return_attn, mems, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: output with shape [128, 1] doesn't match the broadcast shape [1, 128, 128]"
          ]
        }
      ],
      "source": [
        "model(src, tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92978633",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "92978633",
        "outputId": "987a72c7-f896-4f19-86ce-11afe1ede53d"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a9732fcda0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mprint_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTAG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                         overfit_stop=False)\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTASK_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test_results.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/algotrade/NN/run_experiment.py\u001b[0m in \u001b[0;36mtrain_validate_model\u001b[0;34m(model, train_generator, val_generator, optim, model_name, config, generate_every, num_batches, verbose, overfit_stop, print_file, tag, head_start)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/algotrade/NN/x_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/algotrade/NN/x_transformers/x_transformers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_embeddings, mask, return_mems, return_attn, mem, mems, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_mem\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.DoubleTensor instead (while checking arguments for embedding)"
          ]
        }
      ],
      "source": [
        "drive_path = 'drive/MyDrive/stocks_logs/'\n",
        "print_file = f'{drive_path}{TAG}_logs.txt'\n",
        "t = time.time()\n",
        "for init_num in range(NUM_INITS):\n",
        "    with open(print_file, 'a') as f:\n",
        "        f.write('\\n\\nInit number ' + str(init_num)+'\\n')\n",
        "    for i, param in enumerate(list(model_parameters)):\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write('\\n\\n' + str(param)+'\\n')\n",
        "        param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
        "        param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
        "        param.pop('depth,heads')\n",
        "\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write(f'{i / len(model_parameters) * 100}%')\n",
        "        model = XTransformer(**param).cuda()\n",
        "\n",
        "        model_name = f\"{TASK_NAME}{INPUT_LEN}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_{TAG}_v{init_num}\"\n",
        "\n",
        "        optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "        train_validate_model(model, \n",
        "                        train_generator=gen_train, \n",
        "                        val_generator=gen_val, \n",
        "                        optim=optim, \n",
        "                        model_name=model_name, \n",
        "                        config=param,\n",
        "                        num_batches=NUM_BATCHES,\n",
        "                        generate_every=GENERATE_EVERY,\n",
        "                        print_file=print_file,\n",
        "                        tag=TAG,\n",
        "                        overfit_stop=False)\n",
        "        test_model(model, gen_test, model_name, param, TASK_NAME, tag=TAG, log_path=drive_path+'test_results.csv')\n",
        "        with open(print_file, 'a') as f:\n",
        "            f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
        "        t = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xsvN-ny_a_X",
      "metadata": {
        "id": "1xsvN-ny_a_X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b0f5d48a",
      "metadata": {
        "id": "b0f5d48a"
      },
      "source": [
        "### Refit models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbfb752e",
      "metadata": {
        "id": "cbfb752e"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# def load_cpt(config, v, task_name, input_length):\n",
        "#     for fns in os.walk('checkpoints'):\n",
        "#         model_names = fns[2]\n",
        "        \n",
        "#     prefix = '{task_name}_dim{dim}d{d}h{h}M{M}l{l}'\n",
        "#     name = prefix.format(task_name=task_name,\n",
        "#                         dim=config['dim'],\n",
        "#                         d=config['enc_depth'], h=config['enc_heads'], \n",
        "#                         M=config['enc_num_memory_tokens'], \n",
        "#                         l=input_length)\n",
        "\n",
        "#     checkpoint_paths = ['checkpoints/' + n for n in model_names if name in n]\n",
        "#     cpt = torch.load(checkpoint_paths[v])\n",
        "#     bn, model_state, optim_state = cpt['batch_num'], cpt['state_dict'], cpt['optimizer']\n",
        "\n",
        "#     model = XTransformer(**config).cuda()\n",
        "#     model.load_state_dict(model_state)\n",
        "\n",
        "#     optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#     optim.load_state_dict(optim_state)\n",
        "\n",
        "#     return bn, model, optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "396ecf0a",
      "metadata": {
        "id": "396ecf0a"
      },
      "outputs": [],
      "source": [
        "# TAG = 'refit_to_max'\n",
        "# LEARNING_RATE = 0.001\n",
        "\n",
        "# path = f\"checkpoints/{TASK_NAME}{INPUT_LEN}/\"\n",
        "\n",
        "# for name in next(os.walk(path))[2]:\n",
        "#     print(name)\n",
        "#     if name == 'copy24_dim128d2h4M12l12_10tkn_len24_v2_10tkn_len24.pt':\n",
        "#         continue\n",
        "#     cpt = torch.load(path+name)\n",
        "#     print(cpt['batch_num'])\n",
        "#     delta_batches = NUM_BATCHES - cpt['batch_num'] - 1\n",
        "#     if delta_batches < 1:\n",
        "#         continue\n",
        "    \n",
        "#     split = name.split('_')\n",
        "#     config = {'dec_max_seq_len': DEC_SEQ_LEN,\n",
        "#          'dec_num_tokens': NUM_TOKENS,\n",
        "#          'dim': int(split[1].split('dim')[1].split('d')[0]),\n",
        "#          'enc_max_seq_len': int(split[1].split('M')[1].split('l')[1]),\n",
        "#          'enc_num_memory_tokens': int(split[1].split('M')[1].split('l')[0]),\n",
        "#          'enc_num_tokens': NUM_TOKENS,\n",
        "#          'return_tgt_loss': True,\n",
        "#          'tie_token_embeds': True,\n",
        "#          'enc_depth': int(split[1][3:].split('d')[1].split('h')[0]),\n",
        "#          'enc_heads': int(split[1][3:].split('d')[1].split('h')[1].split('M')[0]),\n",
        "#          'dec_depth': int(split[1][3:].split('d')[1].split('h')[0]),\n",
        "#          'dec_heads': int(split[1][3:].split('d')[1].split('h')[1].split('M')[0]),\n",
        "#          'tag': TAG,\n",
        "#          'task_name': TASK_NAME}\n",
        "    \n",
        "    \n",
        "#     gen_train = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_train', batch_size=BATCH_SIZE)\n",
        "#     gen_val = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_val', batch_size=VAL_SIZE)\n",
        "#     gen_test = data_loader(path=f'data{INPUT_LEN}', task_name=f'{TASK_NAME}_test', batch_size=TEST_SIZE)\n",
        "\n",
        "\n",
        "#     print_file = f'logs/{TASK_NAME}_{TAG}_memory_logs.txt'\n",
        "#     t = time.time()\n",
        "#     with torch.cuda.device(0):\n",
        "#         with open(print_file, 'a') as f:\n",
        "#             f.write('\\n\\n' + str(config)+'\\n')\n",
        "#             f.write(str(delta_batches) + ' batches to go.\\n')\n",
        "\n",
        "#         print('\\n\\n' + str(config)+'\\n')\n",
        "#         print(str(delta_batches) + ' batches to go.\\n')\n",
        "#         model_name = name\n",
        "#         model = XTransformer(**config).cuda()\n",
        "#         optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "        \n",
        "#         model.load_state_dict(cpt['state_dict'])\n",
        "#         optim.load_state_dict(cpt['optimizer'])\n",
        "\n",
        "#         train_validate_model(model, \n",
        "#                             train_generator=gen_train, \n",
        "#                             val_generator=gen_val, \n",
        "#                             optim=optim, \n",
        "#                             model_name=model_name, \n",
        "#                             config=config,\n",
        "#                             num_batches=delta_batches,\n",
        "#                             generate_every=GENERATE_EVERY,\n",
        "#                             print_file=print_file,\n",
        "#                             tag=TAG,\n",
        "#                             overfit_stop=False)\n",
        "#         test_model(model, gen_test, model_name, config, TASK_NAME, tag=TAG)\n",
        "\n",
        "#         with open(print_file, 'a') as f:\n",
        "#             f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
        "#         t = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a43009a",
      "metadata": {
        "id": "2a43009a"
      },
      "outputs": [],
      "source": [
        "test_model(model, gen_test, model_name, config, TASK_NAME, tag=TAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981a173d",
      "metadata": {
        "id": "981a173d"
      },
      "outputs": [],
      "source": [
        "# gen_train = data_loader(task_name=f'{TASK_NAME}_train', batch_size=BATCH_SIZE, enc_seq_len=INPUT_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
        "# gen_val = data_loader(task_name=f'{TASK_NAME}_val', batch_size=VAL_SIZE, enc_seq_len=INPUT_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
        "# gen_test = data_loader(task_name=f'{TASK_NAME}_test', batch_size=TEST_SIZE, enc_seq_len=INPUT_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
        "\n",
        "\n",
        "# print_file = f'logs/{TASK_NAME}_{TAG}_memory_logs.txt'\n",
        "# t = time.time()\n",
        "# with torch.cuda.device(0):\n",
        "#     for init_num in range(NUM_INITS):\n",
        "#         with open(print_file, 'a') as f:\n",
        "#             f.write('\\n\\nInit number ' + str(init_num)+'\\n')\n",
        "#         for i, param in enumerate(list(model_parameters)):\n",
        "#             with open(print_file, 'a') as f:\n",
        "#                 f.write('\\n\\n' + str(param)+'\\n')\n",
        "#             param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
        "#             param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
        "#             param.pop('depth,heads')\n",
        "\n",
        "#             with open(print_file, 'a') as f:\n",
        "#                 f.write(f'{i / len(model_parameters) * 100}%')\n",
        "#             model = XTransformer(**param).cuda()\n",
        "\n",
        "#             model_name = f\"{TASK_NAME}{INPUT_LEN}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_v{init_num}\"\n",
        "\n",
        "#             optim = torch.optim.Adam(model.ффparameters(), lr=LEARNING_RATE)\n",
        "            \n",
        "#             bn, model, optim = load_cpt(param, v=init_num, task_name='copy55', input_length=param['enc_max_seq_len'])\n",
        "#             with open(print_file, 'a') as f:\n",
        "#                 f.write(f'BN: {bn}\\n')\n",
        "#             if bn < 130_000:\n",
        "#                 train_validate_model(model, \n",
        "#                                     train_generator=gen_train, \n",
        "#                                     val_generator=gen_val, \n",
        "#                                     optim=optim, \n",
        "#                                     model_name=model_name, \n",
        "#                                     dec_seq_len=DEC_SEQ_LEN,\n",
        "#                                     num_batches=NUM_BATCHES,\n",
        "#                                     generate_every=GENERATE_EVERY,\n",
        "#                                     print_file=print_file,\n",
        "#                                     tag=TAG,\n",
        "#                                     overfit_stop=False,\n",
        "#                                     head_start=(130_000 - bn)/GENERATE_EVERY)\n",
        "#                 test_model(model, gen_test, model_name, param, TASK_NAME, tag=TAG, dec_seq_len=param['dec_max_seq_len'])\n",
        "#             with open(print_file, 'a') as f:\n",
        "#                 f.write(f'\\nTotal time: {time.time() - t}\\n')\n",
        "#             t = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99c92dea",
      "metadata": {
        "id": "99c92dea"
      },
      "outputs": [],
      "source": [
        "from run_experiment import save_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e649122",
      "metadata": {
        "id": "4e649122"
      },
      "outputs": [],
      "source": [
        "# save_path = f'checkpoints/{model_name}_b{i}_{TAG}_maxval.pt'\n",
        "# save_cpt(save_path, model, optim)\n",
        "\n",
        "# if i // generate_every < head_start:\n",
        "#     continue\n",
        "\n",
        "# # early stopping\n",
        "# smoothed_val_scores = [np.mean(validation_scores[i-WINDOW_SIZE+1:i]) for i in range(WINDOW_SIZE-1, len(validation_scores))]\n",
        "\n",
        "# if overfit_stop and max(smoothed_val_scores) > max(smoothed_val_scores[-PATIENCE:]):\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "448fca13",
      "metadata": {
        "id": "448fca13"
      },
      "source": [
        "### Test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09847b7",
      "metadata": {
        "id": "b09847b7"
      },
      "outputs": [],
      "source": [
        "init_num = 0\n",
        "\n",
        "gen_train = data_loader(task_name=f'{TASK_NAME}_train', batch_size=BATCH_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
        "gen_val = data_loader(task_name=f'{TASK_NAME}_val', batch_size=VAL_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
        "gen_test = data_loader(task_name=f'{TASK_NAME}_test', batch_size=TEST_SIZE, enc_seq_len=ENC_SEQ_LEN, dec_seq_len=DEC_SEQ_LEN)\n",
        "\n",
        "\n",
        "param = list(model_parameters)[5]\n",
        "print(param)\n",
        "param['enc_depth'], param['enc_heads'] = param['depth,heads']\n",
        "param['dec_depth'], param['dec_heads'] = param['depth,heads']\n",
        "param.pop('depth,heads')\n",
        "\n",
        "model = XTransformer(**param).cuda()\n",
        "\n",
        "model_name = f\"{TASK_NAME}_dim{param['dim']}d{param['enc_depth']}h{param['enc_heads']}M{param['enc_num_memory_tokens']}l{param['enc_max_seq_len']}_v{init_num}\"\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "src, tgt, _, _ = next(gen_train)\n",
        "\n",
        "print(model.encoder.max_seq_len, model.encoder.num_memory_tokens)\n",
        "model.encoder(torch.cat((src, src)), return_embeddings=True).shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TF_emb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
